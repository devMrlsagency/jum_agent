'''Quality assurance agent.

The `QaAgent` is responsible for checking whether generated code meets the
requirements of the task. At this stage it performs a minimal validation by
compiling and optionally executing the code. In a production system you
would write tests specific to each task and use the language model to
explain any failures.
'''

from __future__ import annotations

import contextlib
import io
import textwrap
from typing import Any, Tuple

from ..models.llm_client import LLMClient


class QaAgent:
    '''Agent that validates generated code.'''

    def __init__(self, llm_client: LLMClient) -> None:
        self.llm = llm_client

    def check_code(self, code: str, task: str) -> Tuple[bool, str]:
        '''Check whether the generated code fulfils the task.

        For now this method simply attempts to compile the code and run it in a
        sandbox. If no exception occurs, the QA passes; otherwise it returns
        the error message. You can replace this with more sophisticated
        evaluations, including running unit tests and using the LLM to
        interpret results.

        :param code: Code generated by the `DevAgent`.
        :param task: Original task description.
        :returns: A tuple `(passed, feedback)`. If `passed` is False, the
            `feedback` contains a brief description of the failure.
        '''
        local_namespace: dict[str, Any] = {}
        try:
            compiled = compile(code, filename='<generated>', mode='exec')
        except Exception as exc:
            return False, f'Compilation failed: {exc}'
        # Capture stdout while running code
        buf = io.StringIO()
        try:
            with contextlib.redirect_stdout(buf):
                exec(compiled, {}, local_namespace)
        except Exception as exc:
            return False, f'Runtime error: {exc}'
        # In a real QA, we would check outputs or behaviours here.
        return True, ''
